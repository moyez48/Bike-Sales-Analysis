{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03f464b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd            # For data loading and manipulation\n",
    "import numpy as np             # Numerical operations\n",
    "from sklearn.model_selection import train_test_split   # For data splitting (train/test)\n",
    "from sklearn.preprocessing import StandardScaler       # For feature scaling/normalization\n",
    "\n",
    "\n",
    "df = pd.read_excel(\"C:\\\\Users\\\\osaze\\\\OneDrive\\\\Desktop\\\\Python\\\\Datasets\\\\Final3.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fcca8fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 383 entries, 0 to 382\n",
      "Data columns (total 51 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   Unnamed: 0        383 non-null    int64  \n",
      " 1   Player            383 non-null    object \n",
      " 2   MV                383 non-null    float64\n",
      " 3   Nation            383 non-null    object \n",
      " 4   Pos               383 non-null    object \n",
      " 5   Club              383 non-null    object \n",
      " 6   Leauge            383 non-null    object \n",
      " 7   Age               383 non-null    int64  \n",
      " 8   MP                383 non-null    int64  \n",
      " 9   Starts            383 non-null    int64  \n",
      " 10  Min               383 non-null    int64  \n",
      " 11  Gls               383 non-null    int64  \n",
      " 12  Ast               383 non-null    int64  \n",
      " 13  PK_x              383 non-null    int64  \n",
      " 14  PKatt_x           383 non-null    int64  \n",
      " 15  CrdY              383 non-null    int64  \n",
      " 16  CrdR              383 non-null    int64  \n",
      " 17  Gls90             383 non-null    float64\n",
      " 18  Ast90             383 non-null    float64\n",
      " 19  G+A               383 non-null    float64\n",
      " 20  Gls+Ast           383 non-null    int64  \n",
      " 21  PK                383 non-null    int64  \n",
      " 22  PKatt             383 non-null    int64  \n",
      " 23  Sh                383 non-null    int64  \n",
      " 24  SoT               383 non-null    int64  \n",
      " 25  FK                383 non-null    int64  \n",
      " 26  SoT%              383 non-null    float64\n",
      " 27  Sh/90             383 non-null    float64\n",
      " 28  SoT/90            383 non-null    float64\n",
      " 29  G/Sh              383 non-null    float64\n",
      " 30  G/SoT             383 non-null    float64\n",
      " 31  Tackle            383 non-null    int64  \n",
      " 32  TackleW           383 non-null    int64  \n",
      " 33  TakleD            383 non-null    int64  \n",
      " 34  Press             383 non-null    int64  \n",
      " 35  Succ_x            383 non-null    int64  \n",
      " 36  %                 383 non-null    float64\n",
      " 37  Blocks            383 non-null    int64  \n",
      " 38  ShotB             383 non-null    int64  \n",
      " 39  PassB             383 non-null    int64  \n",
      " 40  Int               383 non-null    int64  \n",
      " 41  Clr               383 non-null    int64  \n",
      " 42  Passes Completed  383 non-null    int64  \n",
      " 43  Passes Attempted  383 non-null    int64  \n",
      " 44  Cmp%              383 non-null    float64\n",
      " 45  Touches           383 non-null    int64  \n",
      " 46  Succ_y            383 non-null    int64  \n",
      " 47  Att               383 non-null    int64  \n",
      " 48  Succ%             383 non-null    float64\n",
      " 49  #Pl               383 non-null    int64  \n",
      " 50  NR                383 non-null    int64  \n",
      "dtypes: float64(12), int64(34), object(5)\n",
      "memory usage: 152.7+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'Player', 'MV', 'Nation', 'Pos', 'Club', 'Leauge', 'Age',\n",
       "       'MP', 'Starts', 'Min', 'Gls', 'Ast', 'PK_x', 'PKatt_x', 'CrdY', 'CrdR',\n",
       "       'Gls90', 'Ast90', 'G+A', 'Gls+Ast', 'PK', 'PKatt', 'Sh', 'SoT', 'FK',\n",
       "       'SoT%', 'Sh/90', 'SoT/90', 'G/Sh', 'G/SoT', 'Tackle', 'TackleW',\n",
       "       'TakleD', 'Press', 'Succ_x', '%', 'Blocks', 'ShotB', 'PassB', 'Int',\n",
       "       'Clr', 'Passes Completed', 'Passes Attempted', 'Cmp%', 'Touches',\n",
       "       'Succ_y', 'Att', 'Succ%', '#Pl', 'NR', 'League'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape\n",
    "df.info()\n",
    "df.head(3)\n",
    "df.describe()\n",
    "df.columns\n",
    "\n",
    "df['League'] = df['Leauge']\n",
    "\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01c8c657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use 'MV' for market value, drop irrelevant columns\n",
    "exclude_columns = ['MV', 'Player', 'Nation', 'Pos', 'Club', 'League']\n",
    "X = df.drop(exclude_columns, axis=1, errors='ignore').select_dtypes(include=[np.number])\n",
    "y = df['MV']\n",
    "\n",
    "# Split data into training and testing sets, then scale features\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d67b27ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal alpha (lambda): 0.3176482767179257\n",
      "Unnamed: 0: -9.0506\n",
      "Age: -1.0188\n",
      "MP: -3.5700\n",
      "Starts: -0.0000\n",
      "Min: -0.0000\n",
      "Gls: 0.7867\n",
      "Ast: 0.0000\n",
      "PK_x: -0.0000\n",
      "PKatt_x: -0.2184\n",
      "CrdY: -0.0000\n",
      "CrdR: 0.0000\n",
      "Gls90: 4.5766\n",
      "Ast90: 0.7154\n",
      "G+A: 0.0000\n",
      "Gls+Ast: 2.9710\n",
      "PK: -0.2317\n",
      "PKatt: -0.0000\n",
      "Sh: 0.0000\n",
      "SoT: 0.2321\n",
      "FK: 0.2698\n",
      "SoT%: 0.0000\n",
      "Sh/90: 0.0000\n",
      "SoT/90: 0.0000\n",
      "G/Sh: 0.0000\n",
      "G/SoT: -1.3371\n",
      "Tackle: 0.0000\n",
      "TackleW: 0.0000\n",
      "TakleD: -0.0000\n",
      "Press: 0.0000\n",
      "Succ_x: 0.0000\n",
      "%: 1.3949\n",
      "Blocks: -0.0000\n",
      "ShotB: 0.0000\n",
      "PassB: -1.2704\n",
      "Int: -0.0000\n",
      "Clr: -0.0000\n",
      "Passes Completed: 6.0155\n",
      "Passes Attempted: 0.0000\n",
      "Cmp%: 1.6661\n",
      "Touches: 0.0000\n",
      "Succ_y: 0.0000\n",
      "Att: 3.4868\n",
      "Succ%: 0.2621\n",
      "#Pl: 0.0000\n",
      "NR: 0.4196\n",
      "Selected features: ['Unnamed: 0', 'Age', 'MP', 'Gls', 'PKatt_x', 'Gls90', 'Ast90', 'Gls+Ast', 'PK', 'SoT', 'FK', 'G/SoT', '%', 'PassB', 'Passes Completed', 'Cmp%', 'Att', 'Succ%', 'NR']\n",
      "Selected features: ['Unnamed: 0', 'Age', 'MP', 'Gls', 'PKatt_x', 'Gls90', 'Ast90', 'Gls+Ast', 'PK', 'SoT', 'FK', 'G/SoT', '%', 'PassB', 'Passes Completed', 'Cmp%', 'Att', 'Succ%', 'NR']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\osaze\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.199e+01, tolerance: 9.826e+00\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\osaze\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.706e+01, tolerance: 9.826e+00\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\osaze\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.752e+01, tolerance: 9.826e+00\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\osaze\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.564e+01, tolerance: 9.969e+00\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\osaze\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.532e+01, tolerance: 9.969e+00\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\osaze\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.944e+01, tolerance: 9.969e+00\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\osaze\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.238e+01, tolerance: 9.969e+00\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\osaze\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.064e+01, tolerance: 9.969e+00\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\osaze\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.245e+01, tolerance: 9.969e+00\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\osaze\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.624e+01, tolerance: 9.343e+00\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\osaze\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.683e+01, tolerance: 9.343e+00\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\osaze\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.645e+01, tolerance: 9.343e+00\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\osaze\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.594e+01, tolerance: 9.343e+00\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\osaze\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.512e+01, tolerance: 9.343e+00\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\osaze\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.436e+01, tolerance: 9.343e+00\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\osaze\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.388e+01, tolerance: 9.343e+00\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\osaze\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.408e+01, tolerance: 9.875e+00\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\osaze\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.607e+01, tolerance: 9.875e+00\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\osaze\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.811e+01, tolerance: 9.875e+00\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\osaze\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.806e+01, tolerance: 9.875e+00\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\osaze\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.759e+01, tolerance: 9.875e+00\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\osaze\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.705e+01, tolerance: 9.875e+00\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\osaze\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.649e+01, tolerance: 9.875e+00\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\osaze\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.593e+01, tolerance: 9.875e+00\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\osaze\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.537e+01, tolerance: 9.875e+00\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n"
     ]
    }
   ],
   "source": [
    "#fit lasso with cross-validation to find optimal lambda\n",
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "lasso = LassoCV(cv=5, random_state=42)\n",
    "lasso.fit(X_train_scaled, y_train)\n",
    "# attribute is alpha_ (with trailing underscore) after fitting\n",
    "print(\"Optimal alpha (lambda):\", lasso.alpha_)\n",
    "\n",
    "# Get coefficients and selected features\n",
    "coefficients = lasso.coef_\n",
    "feature_names = X.columns\n",
    "# Combine feature names and coefficients for easy inspection\n",
    "for name, coef in zip(feature_names, coefficients):\n",
    "    print(f\"{name}: {coef:.4f}\")\n",
    "selected_features = [name for name, coef in zip(feature_names, coefficients) if coef != 0]\n",
    "print(\"Selected features:\", selected_features)\n",
    "\n",
    "#filtering for non-zero coefficients\n",
    "thereshold = 0.00001\n",
    "selected_features = [name for name, coef in zip(feature_names, coefficients) if abs(coef) > thereshold]\n",
    "print(\"Selected features:\", selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73684f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression Training MSE: 154.4725447311649\n",
      "Linear Regression Training R^2: 0.6146972541222995\n",
      "Linear Regression MSE: 344.08208760905785\n",
      "Linear Regression R^2: 0.5192405796003305\n"
     ]
    }
   ],
   "source": [
    "# Linear regression with variables selected by Lasso\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "lr = LinearRegression()\n",
    "X_train_sel = X_train[selected_features]\n",
    "X_test_sel = X_test[selected_features]\n",
    "lr.fit(X_train_sel, y_train)\n",
    "\n",
    "#training R^2 and MSE\n",
    "# Linear regression with variables selected by Lasso\n",
    "\n",
    "lr = LinearRegression()\n",
    "X_train_sel = X_train[selected_features]\n",
    "X_test_sel = X_test[selected_features]\n",
    "lr.fit(X_train_sel, y_train)\n",
    "\n",
    "#training R^2 and MSE\n",
    "mse_train_lr = mean_squared_error(y_train, lr.predict(X_train_sel))\n",
    "r2_train_lr = lr.score(X_train_sel, y_train)\n",
    "print(\"Linear Regression Training MSE:\", mse_train_lr)\n",
    "print(\"Linear Regression Training R^2:\", r2_train_lr)\n",
    "\n",
    "#testing R^2 and MSE\n",
    "y_pred_lr = lr.predict(X_test_sel)\n",
    "mse_lr = mean_squared_error(y_test, y_pred_lr)\n",
    "r2_lr = r2_score(y_test, y_pred_lr)\n",
    "print(\"Linear Regression MSE:\", mse_lr)\n",
    "print(\"Linear Regression R^2:\", r2_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9190ccc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Training MSE: 6.309844994736041e-07\n",
      "XGBoost Training R^2: 0.9999999984261277\n",
      "XGBoost MSE: 47.64497771785833\n",
      "XGBoost R^2: 0.9334293394004921\n"
     ]
    }
   ],
   "source": [
    "# Boosting tree-based model (XGBoost) with selected features\n",
    "from xgboost import XGBRegressor\n",
    "xgb = XGBRegressor(random_state=42)\n",
    "xgb.fit(X_train_sel, y_train)\n",
    "\n",
    "#training R^2 and MSE for XGBoost\n",
    "mse_train_xgb = mean_squared_error(y_train, xgb.predict(X_train_sel))\n",
    "r2_train_xgb = xgb.score(X_train_sel, y_train)\n",
    "print(\"XGBoost Training MSE:\", mse_train_xgb)\n",
    "print(\"XGBoost Training R^2:\", r2_train_xgb)\n",
    "\n",
    "#testing R^2 and MSE for XGBoost\n",
    "y_pred_xgb = xgb.predict(X_test_sel)\n",
    "mse_xgb = mean_squared_error(y_test, y_pred_xgb)\n",
    "r2_xgb = r2_score(y_test, y_pred_xgb)\n",
    "print(\"XGBoost MSE:\", mse_xgb)\n",
    "print(\"XGBoost R^2:\", r2_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fac0f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the initial boosting tree model overfitted the training data significantly compared to the linear regression model.\n",
    "# this could be due to lack of stopping parameters like max_depth, n_estimators, learning_rate etc and regularization.\n",
    "\n",
    "# Create a small validation split from the selected training features for early stopping\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Use the feature-selected training frame (X_train_sel) created earlier\n",
    "# Split a validation set from the training data (20% of train by default)\n",
    "X_train_sub, X_val, y_train_sub, y_val = train_test_split(X_train_sel, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build DMatrix objects from the split sets (use X_test_sel for final test predictions)\n",
    "dtrain = xgb.DMatrix(X_train_sub, label=y_train_sub)\n",
    "dval = xgb.DMatrix(X_val, label=y_val)\n",
    "dtest = xgb.DMatrix(X_test_sel)\n",
    "\n",
    "# Improved params: faster learning rate, stronger regularization, shallower trees\n",
    "params = {\n",
    "    'objective': 'reg:squarederror',\n",
    "    'max_depth': 3,              # Shallower trees (was 4)\n",
    "    'eta': 0.1,                  # Faster learning (was 0.05) so early stopping can work\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'alpha': 0.5,                # Stronger L1 regularization (was 0.1)\n",
    "    'lambda': 2.0,               # Stronger L2 regularization (was 1.0)\n",
    "    'min_child_weight': 3,       # Added: prevents overfitting on small leaf nodes\n",
    "    'eval_metric': 'rmse',\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "evallist = [(dval, 'eval'), (dtrain, 'train')]\n",
    "\n",
    "bst = xgb.train(\n",
    "    params,\n",
    "    dtrain,\n",
    "    num_boost_round=1000,\n",
    "    evals=evallist,\n",
    "    early_stopping_rounds=20,    # Reduced from 50 for faster stopping\n",
    "    verbose_eval=50               # Print progress every 50 rounds\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining stopped at iteration: {bst.best_iteration}\")\n",
    "\n",
    "# Predictions on the training-subset and test set\n",
    "y_pred_train = bst.predict(xgb.DMatrix(X_train_sub))\n",
    "y_pred_test = bst.predict(dtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6d916e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Tuned XGBoost Performance ---\n",
      "Training MSE: 0.0001797724030028967\n",
      "Training R^2: 0.9999995648042408\n",
      "\n",
      "Test MSE: 73.02955928144452\n",
      "Test R^2: 0.8979614171834434\n",
      "\n",
      "Model stopped at iteration: 999\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Evaluate the tuned XGBoost model's performance\n",
    "print(\"--- Tuned XGBoost Performance ---\")\n",
    "# compute training metrics using the training subset used for the booster\n",
    "print(\"Training MSE:\", mean_squared_error(y_train_sub, y_pred_train))\n",
    "print(\"Training R^2:\", r2_score(y_train_sub, y_pred_train))\n",
    "print(\"\\nTest MSE:\", mean_squared_error(y_test, y_pred_test))\n",
    "print(\"Test R^2:\", r2_score(y_test, y_pred_test))\n",
    "\n",
    "print(\"\\nModel stopped at iteration:\", bst.best_iteration)\n",
    "\n",
    "# DO NOT overwrite y_train here - keep it as the original 306 samples for later models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e0592d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting hyperparameter tuning with RandomizedSearchCV...\n",
      "Training on full dataset: X_train_sel shape = (306, 19), y_train shape = (306,)\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "\n",
      "Hyperparameter tuning finished.\n",
      "Best parameters found:  {'subsample': np.float64(0.7), 'reg_lambda': 3, 'reg_alpha': 1, 'n_estimators': 300, 'min_child_weight': 3, 'max_depth': 4, 'learning_rate': 0.05, 'colsample_bytree': np.float64(0.8999999999999999)}\n",
      "\n",
      "--- Performance of Final, Tuned Model ---\n",
      "Training MSE: 0.4253018220948475\n",
      "Training R^2: 0.9989391644957677\n",
      "\n",
      "Test MSE: 56.695166819013565\n",
      "Test R^2: 0.9207842066735531\n",
      "\n",
      "Hyperparameter tuning finished.\n",
      "Best parameters found:  {'subsample': np.float64(0.7), 'reg_lambda': 3, 'reg_alpha': 1, 'n_estimators': 300, 'min_child_weight': 3, 'max_depth': 4, 'learning_rate': 0.05, 'colsample_bytree': np.float64(0.8999999999999999)}\n",
      "\n",
      "--- Performance of Final, Tuned Model ---\n",
      "Training MSE: 0.4253018220948475\n",
      "Training R^2: 0.9989391644957677\n",
      "\n",
      "Test MSE: 56.695166819013565\n",
      "Test R^2: 0.9207842066735531\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# --- 1. Define the Hyperparameter Grid ---\n",
    "# Adjusted to favor simpler models (shallower trees, stronger regularization)\n",
    "param_dist = {\n",
    "    'max_depth': [2, 3, 4, 5],              # Removed 6, 7 (too deep for small dataset)\n",
    "    'learning_rate': [0.05, 0.1, 0.15],     # Removed 0.01 (too slow)\n",
    "    'n_estimators': [100, 300, 500, 800],   # Lower range (fewer trees = less overfitting)\n",
    "    'subsample': np.arange(0.7, 1.0, 0.1),  # Higher minimum (more data per tree)\n",
    "    'colsample_bytree': np.arange(0.7, 1.0, 0.1),\n",
    "    'reg_alpha': [0.5, 1, 2],               # Stronger L1 regularization\n",
    "    'reg_lambda': [2, 3, 5],                # Stronger L2 regularization\n",
    "    'min_child_weight': [3, 5, 7]           # Added: prevent small leaf nodes\n",
    "}\n",
    "\n",
    "# --- 2. Set up RandomizedSearchCV ---\n",
    "xgb_reg = XGBRegressor(objective='reg:squarederror', random_state=42)\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=xgb_reg,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=50,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    cv=5,\n",
    "    verbose=1,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# --- 3. Run the Search ---\n",
    "# FIX: Get the original y_train from the initial split (not the reassigned y_train_sub)\n",
    "from sklearn.model_selection import train_test_split\n",
    "_, _, y_train_original, _ = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Starting hyperparameter tuning with RandomizedSearchCV...\")\n",
    "print(f\"Training on full dataset: X_train_sel shape = {X_train_sel.shape}, y_train shape = {y_train_original.shape}\")\n",
    "random_search.fit(X_train_sel, y_train_original)\n",
    "\n",
    "# --- 4. Get the Best Model and Evaluate ---\n",
    "print(\"\\nHyperparameter tuning finished.\")\n",
    "print(\"Best parameters found: \", random_search.best_params_)\n",
    "\n",
    "best_xgb_tuned = random_search.best_estimator_\n",
    "\n",
    "# Evaluate on training set (full training data)\n",
    "y_pred_train_tuned = best_xgb_tuned.predict(X_train_sel)\n",
    "print(\"\\n--- Performance of Final, Tuned Model ---\")\n",
    "print(\"Training MSE:\", mean_squared_error(y_train_original, y_pred_train_tuned))\n",
    "print(\"Training R^2:\", r2_score(y_train_original, y_pred_train_tuned))\n",
    "\n",
    "# Evaluate on test set\n",
    "y_pred_test_tuned = best_xgb_tuned.predict(X_test_sel)\n",
    "print(\"\\nTest MSE:\", mean_squared_error(y_test, y_pred_test_tuned))\n",
    "print(\"Test R^2:\", r2_score(y_test, y_pred_test_tuned))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6afc2dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Random Forest...\n",
      "Using 306 training samples with 19 Lasso-selected features\n",
      "\n",
      "--- Random Forest Performance ---\n",
      "Training MSE: 4.985988011697114\n",
      "Training R^2: 0.9875633894996448\n",
      "\n",
      "Test MSE: 59.79141169902284\n",
      "Test R^2: 0.916458061990253\n",
      "\n",
      "--- Feature Importance (What Drives Market Value?) ---\n",
      "         feature  importance\n",
      "      Unnamed: 0    0.955511\n",
      "             Age    0.006758\n",
      "           Ast90    0.006572\n",
      "           Gls90    0.005123\n",
      "             Gls    0.004198\n",
      "           Succ%    0.002962\n",
      "Passes Completed    0.002867\n",
      "             Att    0.002609\n",
      "            Cmp%    0.002013\n",
      "           G/SoT    0.001673\n",
      "              PK    0.001621\n",
      "              NR    0.001526\n",
      "              MP    0.001307\n",
      "         Gls+Ast    0.001282\n",
      "              FK    0.001135\n",
      "               %    0.001107\n",
      "           PassB    0.000802\n",
      "             SoT    0.000502\n",
      "         PKatt_x    0.000432\n",
      "\n",
      "Top 3 predictors explain 96.9% of the model's decisions\n",
      "\n",
      "--- Random Forest Performance ---\n",
      "Training MSE: 4.985988011697114\n",
      "Training R^2: 0.9875633894996448\n",
      "\n",
      "Test MSE: 59.79141169902284\n",
      "Test R^2: 0.916458061990253\n",
      "\n",
      "--- Feature Importance (What Drives Market Value?) ---\n",
      "         feature  importance\n",
      "      Unnamed: 0    0.955511\n",
      "             Age    0.006758\n",
      "           Ast90    0.006572\n",
      "           Gls90    0.005123\n",
      "             Gls    0.004198\n",
      "           Succ%    0.002962\n",
      "Passes Completed    0.002867\n",
      "             Att    0.002609\n",
      "            Cmp%    0.002013\n",
      "           G/SoT    0.001673\n",
      "              PK    0.001621\n",
      "              NR    0.001526\n",
      "              MP    0.001307\n",
      "         Gls+Ast    0.001282\n",
      "              FK    0.001135\n",
      "               %    0.001107\n",
      "           PassB    0.000802\n",
      "             SoT    0.000502\n",
      "         PKatt_x    0.000432\n",
      "\n",
      "Top 3 predictors explain 96.9% of the model's decisions\n"
     ]
    }
   ],
   "source": [
    "# --- Random Forest Comparison ---\n",
    "# Random Forest is often more robust to overfitting than gradient boosting\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# CRITICAL: Get the original y_train from the initial split (306 samples, not the 244 from y_train_sub)\n",
    "# This ensures fair comparison with the untuned XGBoost which used 306 samples\n",
    "_, _, y_train_full, _ = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a baseline Random Forest with reasonable defaults\n",
    "rf = RandomForestRegressor(\n",
    "    n_estimators=500,\n",
    "    max_depth=10,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=2,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"\\nTraining Random Forest...\")\n",
    "print(f\"Using {X_train_sel.shape[0]} training samples with {X_train_sel.shape[1]} Lasso-selected features\")\n",
    "rf.fit(X_train_sel, y_train_full)\n",
    "\n",
    "# Evaluate on training and test sets\n",
    "y_pred_train_rf = rf.predict(X_train_sel)\n",
    "y_pred_test_rf = rf.predict(X_test_sel)\n",
    "\n",
    "print(\"\\n--- Random Forest Performance ---\")\n",
    "print(\"Training MSE:\", mean_squared_error(y_train_full, y_pred_train_rf))\n",
    "print(\"Training R^2:\", r2_score(y_train_full, y_pred_train_rf))\n",
    "print(\"\\nTest MSE:\", mean_squared_error(y_test, y_pred_test_rf))\n",
    "print(\"Test R^2:\", r2_score(y_test, y_pred_test_rf))\n",
    "\n",
    "# Feature importance - this tells you which predictors matter most for market value!\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': selected_features,\n",
    "    'importance': rf.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\n--- Feature Importance (What Drives Market Value?) ---\")\n",
    "print(feature_importance.to_string(index=False))\n",
    "print(f\"\\nTop 3 predictors explain {feature_importance.head(3)['importance'].sum():.1%} of the model's decisions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf12324d",
   "metadata": {},
   "source": [
    "## Model Comparison Summary\n",
    "\n",
    "This analysis built a player market value prediction system using Lasso feature selection followed by multiple regression approaches.\n",
    "\n",
    "**Key Findings:**\n",
    "- Started with all available features, used Lasso to identify the most important predictors\n",
    "- Compared Linear Regression, XGBoost (default), XGBoost (tuned), and Random Forest\n",
    "- All models trained on the same Lasso-selected features for fair comparison\n",
    "\n",
    "**Next:** Compare test R² scores to select the best model for my \"custom Transfermarkt\" system!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f02f28a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      " FINAL MODEL COMPARISON - Player Market Value Prediction\n",
      "======================================================================\n",
      "\n",
      "Dataset: 306 training samples, 77 test samples\n",
      "Features: 19 Lasso-selected predictors\n",
      "\n",
      "                       Model  Test R²   Test MSE\n",
      "           XGBoost (Default) 0.933429  47.644978\n",
      "XGBoost (RandomizedSearchCV) 0.920784  56.695167\n",
      "               Random Forest 0.916458  59.791412\n",
      "     XGBoost (Manual Tuning) 0.897961  73.029559\n",
      "           Linear Regression 0.519241 344.082088\n",
      "\n",
      " WINNER: XGBoost (Default)\n",
      "   → Explains 93.3% of market value variance on unseen players\n",
      "   → Ready for production as your custom Transfermarkt system!\n",
      "\n",
      "======================================================================\n",
      " KEY MARKET VALUE PREDICTORS (from Lasso selection)\n",
      "======================================================================\n",
      "\n",
      "Selected Features:\n",
      " 1. Unnamed: 0\n",
      " 2. Age\n",
      " 3. MP\n",
      " 4. Gls\n",
      " 5. PKatt_x\n",
      " 6. Gls90\n",
      " 7. Ast90\n",
      " 8. Gls+Ast\n",
      " 9. PK\n",
      "10. SoT\n",
      "11. FK\n",
      "12. G/SoT\n",
      "13. %\n",
      "14. PassB\n",
      "15. Passes Completed\n",
      "16. Cmp%\n",
      "17. Att\n",
      "18. Succ%\n",
      "19. NR\n",
      "\n",
      "These 19 features capture the essence of player market value!\n"
     ]
    }
   ],
   "source": [
    "# --- Final Model Comparison ---\n",
    "# Compare all models on the same test set\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\" FINAL MODEL COMPARISON - Player Market Value Prediction\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nDataset: {len(y_train_original)} training samples, {len(y_test)} test samples\")\n",
    "print(f\"Features: {len(selected_features)} Lasso-selected predictors\\n\")\n",
    "\n",
    "# Collect results (you'll need to run all cells above first)\n",
    "results = pd.DataFrame({\n",
    "    'Model': [\n",
    "        'Linear Regression',\n",
    "        'XGBoost (Default)',\n",
    "        'XGBoost (Manual Tuning)',\n",
    "        'XGBoost (RandomizedSearchCV)',\n",
    "        'Random Forest'\n",
    "    ],\n",
    "    'Test R²': [\n",
    "        r2_lr,\n",
    "        r2_xgb,\n",
    "        r2_score(y_test, y_pred_test),  # Manual tuned XGBoost\n",
    "        r2_score(y_test, y_pred_test_tuned),  # RandomizedSearchCV\n",
    "        r2_score(y_test, y_pred_test_rf)  # Random Forest\n",
    "    ],\n",
    "    'Test MSE': [\n",
    "        mse_lr,\n",
    "        mse_xgb,\n",
    "        mean_squared_error(y_test, y_pred_test),\n",
    "        mean_squared_error(y_test, y_pred_test_tuned),\n",
    "        mean_squared_error(y_test, y_pred_test_rf)\n",
    "    ]\n",
    "}).sort_values('Test R²', ascending=False)\n",
    "\n",
    "print(results.to_string(index=False))\n",
    "\n",
    "best_model = results.iloc[0]['Model']\n",
    "best_r2 = results.iloc[0]['Test R²']\n",
    "print(f\"\\n WINNER: {best_model}\")\n",
    "print(f\"   → Explains {best_r2:.1%} of market value variance on unseen players\")\n",
    "print(f\"   → Ready for production as your custom Transfermarkt system!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" KEY MARKET VALUE PREDICTORS (from Lasso selection)\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nSelected Features:\")\n",
    "for i, feat in enumerate(selected_features, 1):\n",
    "    print(f\"{i:2d}. {feat}\")\n",
    "print(f\"\\nThese {len(selected_features)} features capture the essence of player market value!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
